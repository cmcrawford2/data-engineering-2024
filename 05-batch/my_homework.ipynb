{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4c6d72-fa69-45e4-818d-da0eb3c4dc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 10:38:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee34a286-a4eb-4731-914d-13ccbbd33aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7433fec2-d018-4401-803a-c925a8040adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1897493\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e6c524-9201-4867-a456-eadf73d04cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatching_base_num       object\n",
      "pickup_datetime            object\n",
      "dropOff_datetime           object\n",
      "PUlocationID              float64\n",
      "DOlocationID              float64\n",
      "SR_Flag                   float64\n",
      "Affiliated_base_number     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "schema = df.dtypes\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f1b6f9-5141-4730-b813-f93b838ce34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc89977b-3bff-4328-a867-c97195ecf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "fhv_schema = types.StructType([\n",
    "    types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropOff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"PUlocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOlocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.StringType(), True),\n",
    "    types.StructField(\"Affiliated_base_number\", types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c6aee2-e0dd-47ea-a6c3-a28756c4ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-03 19:16:36--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240303%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240303T191636Z&X-Amz-Expires=300&X-Amz-Signature=92c1a8f2f312b587a1f0ad0e6dd9392ce3d5a637effeb07eebc086747bfbca72&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-03 19:16:36--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240303%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240303T191636Z&X-Amz-Expires=300&X-Amz-Signature=92c1a8f2f312b587a1f0ad0e6dd9392ce3d5a637effeb07eebc086747bfbca72&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘fhv_tripdata_2019-10.csv.gz’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  18.48M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-03-03 19:16:36 (319 MB/s) - ‘fhv_tripdata_2019-10.csv.gz’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz'\n",
    "!cp fhv_tripdata_2019-10.csv.gz /home/cris/data-engineering-zoomcamp/05-batch/code/data/raw/fhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf0b522d-79ff-45f1-afa3-995a312abbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_path = 'data/raw/fhv/'\n",
    "output_path = 'data/pq/fhv/'\n",
    "\n",
    "df_fhv = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(fhv_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "df_fhv \\\n",
    "        .repartition(6) \\\n",
    "        .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a001e208-8078-4869-a7c9-7e4ba98cc03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv = spark.read.parquet('data/pq/fhv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb89af9-dcca-4a18-b152-35037bda108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cris/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_fhv.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14570dd6-2e7b-4c9c-980c-de6666ceaae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(*)\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE\n",
    "    pickup_datetime > \"2019-10-14 23:59:59\" AND pickup_datetime < \"2019-10-16 00:00:00\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06df8d7-36cb-4337-8211-5e9c1074b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def sane_date(start_date, end_date):\n",
    "    # start_date = datetime.strptime(start_date_str, '%Y-%m-%d %H:%M:%S')\n",
    "    # end_date = datetime.strptime(end_date_str, '%Y-%m-%d %H:%M:%S')\n",
    "    if (end_date.year != start_date.year):\n",
    "        return 0\n",
    "    return (end_date - start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63d254ca-5319-4389-b744-c087e2e7b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "sane_date_udf = F.udf(sane_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e11a3686-1847-4ce1-b7db-310c45b32445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+\n",
      "|       trip_duration|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+--------------------+-------------------+-------------------+------------+------------+\n",
      "|Timedelta: 0 days...|2019-10-01 09:55:38|2019-10-01 10:05:43|          89|          85|\n",
      "|Timedelta: 0 days...|2019-10-21 04:15:47|2019-10-21 04:36:04|         264|         264|\n",
      "|Timedelta: 0 days...|2019-10-19 12:00:00|2019-10-19 12:20:00|         264|         264|\n",
      "|Timedelta: 0 days...|2019-10-11 14:28:00|2019-10-11 14:32:44|         264|         216|\n",
      "|Timedelta: 0 days...|2019-10-21 18:00:26|2019-10-21 18:07:21|         264|          80|\n",
      "|Timedelta: 0 days...|2019-10-03 19:30:35|2019-10-03 20:27:57|         161|          14|\n",
      "|Timedelta: 0 days...|2019-10-25 06:10:40|2019-10-25 06:29:43|         264|         208|\n",
      "|Timedelta: 0 days...|2019-10-30 06:18:02|2019-10-30 06:35:12|         260|         260|\n",
      "|Timedelta: 0 days...|2019-10-09 03:39:16|2019-10-09 03:42:07|         264|         254|\n",
      "|Timedelta: 0 days...|2019-10-27 11:54:37|2019-10-27 12:28:18|         264|         264|\n",
      "|Timedelta: 0 days...|2019-10-14 18:41:24|2019-10-14 19:02:06|         261|         186|\n",
      "|Timedelta: 0 days...|2019-10-19 11:42:45|2019-10-19 12:42:42|         264|          23|\n",
      "|Timedelta: 0 days...|2019-10-17 15:20:32|2019-10-17 15:25:51|         264|         167|\n",
      "|Timedelta: 0 days...|2019-10-21 05:00:47|2019-10-21 05:03:44|          47|         169|\n",
      "|Timedelta: 0 days...|2019-10-14 11:34:00|2019-10-14 13:12:00|         264|         264|\n",
      "|Timedelta: 0 days...|2019-10-13 07:08:41|2019-10-13 07:13:43|         264|         188|\n",
      "|Timedelta: 0 days...|2019-10-24 06:05:22|2019-10-24 06:15:03|         264|         174|\n",
      "|Timedelta: 0 days...|2019-10-20 09:49:18|2019-10-20 10:40:59|         264|         264|\n",
      "|Timedelta: 0 days...|2019-10-06 17:17:28|2019-10-06 18:12:14|         264|         132|\n",
      "|Timedelta: 0 days...|2019-10-07 11:25:57|2019-10-07 11:39:15|         264|         185|\n",
      "+--------------------+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv = df_fhv \\\n",
    "    .withColumn('trip_duration', sane_date_udf(df_fhv.pickup_datetime, df_fhv.dropOff_datetime)) \\\n",
    "    .select('trip_duration', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')\n",
    "df_fhv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e311b3ac-cdcd-40b6-b063-fd9e9048e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|     trip_duration|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|              12.0|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|1.8833333333333333|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|25.616666666666667|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|               1.3|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|               5.3|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |               6.4|\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         | 6.033333333333333|\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         | 9.733333333333333|\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |14.616666666666667|\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         | 6.166666666666667|\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   null|       B00021         |20.583333333333332|\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   null|                B00037|               7.7|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   null|                B00037|12.216666666666667|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   null|                B00037|11.116666666666667|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   null|                B00037|11.416666666666666|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   null|                B00037|               4.8|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   null|                B00037|              14.3|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   null|                B00037| 9.166666666666666|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   null|                B00037|20.583333333333332|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   null|                  #N/A|             47.65|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column for trip length\n",
    "from pyspark.sql.functions import col\n",
    "df_fhv = df_fhv.withColumn(\"trip_duration\", (col(\"dropoff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\")) / 60) # Duration in minutes\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "df_fhv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84fa06d4-078a-4f40-8690-7684f0fd8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "max_trip_duration = df_fhv.agg({\"trip_duration\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d76cf013-447c-48e8-aa31-54902fc82283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timedelta: 9 days, 995 seconds, 0 microseconds (total: 778595 seconds)\n"
     ]
    }
   ],
   "source": [
    "print(max_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4df52999-e9ea-4b55-9380-774f393ee5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.2763888888889\n"
     ]
    }
   ],
   "source": [
    "print(778595/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2da41050-476f-4ef3-a1ad-cfb24d498673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this using sql\n",
    "df_fhv.registerTempTable('trips_data_with_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fd6910d-9b87-4f8b-87ef-20dd45708fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max(trip_duration)|\n",
      "+------------------+\n",
      "|        3.786915E7|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT MAX(trip_duration)\n",
    "    FROM trips_data_with_duration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4696d132-0807-42d2-ae59-e19140dfcda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631152.5\n"
     ]
    }
   ],
   "source": [
    "print(3.786915E7/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebaad93d-49b1-4f34-ab89-e9320f9c9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving on\n",
    "df_zones = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv('zones.csv')\n",
    "\n",
    "df_zones.write.parquet('data/zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f058bed6-b78a-4d09-a00b-ebd82c4f5004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(LocationID='1', Borough='EWR', Zone='Newark Airport', service_zone='EWR'),\n",
       " Row(LocationID='2', Borough='Queens', Zone='Jamaica Bay', service_zone='Boro Zone'),\n",
       " Row(LocationID='3', Borough='Bronx', Zone='Allerton/Pelham Gardens', service_zone='Boro Zone'),\n",
       " Row(LocationID='4', Borough='Manhattan', Zone='Alphabet City', service_zone='Yellow Zone'),\n",
       " Row(LocationID='5', Borough='Staten Island', Zone='Arden Heights', service_zone='Boro Zone'),\n",
       " Row(LocationID='6', Borough='Staten Island', Zone='Arrochar/Fort Wadsworth', service_zone='Boro Zone'),\n",
       " Row(LocationID='7', Borough='Queens', Zone='Astoria', service_zone='Boro Zone'),\n",
       " Row(LocationID='8', Borough='Queens', Zone='Astoria Park', service_zone='Boro Zone'),\n",
       " Row(LocationID='9', Borough='Queens', Zone='Auburndale', service_zone='Boro Zone'),\n",
       " Row(LocationID='10', Borough='Queens', Zone='Baisley Park', service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99efbd1f-650f-4e38-b947-c6d94674df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('data/zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05be3101-1ab5-4476-bfa7-d6dd61639996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e50a999-c034-4306-886a-a8c378b749d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B02784', pickup_datetime=datetime.datetime(2019, 10, 1, 9, 55, 38), dropOff_datetime=datetime.datetime(2019, 10, 1, 10, 5, 43), PUlocationID=89, DOlocationID=85, SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B02429', pickup_datetime=datetime.datetime(2019, 10, 21, 4, 15, 47), dropOff_datetime=datetime.datetime(2019, 10, 21, 4, 36, 4), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B02429')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "368f28d9-c7a3-448f-90bb-14290c50f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_fhv.join(df_zones, df_fhv.PUlocationID == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b3ba46e-41a7-4fdd-80c0-2ab17e98faec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropOff_datetime: timestamp, PUlocationID: int, DOlocationID: int, SR_Flag: string, Affiliated_base_number: string, LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5a34134-beb2-4058-8658-d1ad4de65ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.registerTempTable('trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4af03cd2-66e4-4b94-b298-923d72236ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|                Zone|pickup_count|\n",
      "+--------------------+------------+\n",
      "|         Jamaica Bay|           1|\n",
      "|Governor's Island...|           2|\n",
      "| Green-Wood Cemetery|           5|\n",
      "|       Broad Channel|           8|\n",
      "|     Highbridge Park|          14|\n",
      "|        Battery Park|          15|\n",
      "|Saint Michaels Ce...|          23|\n",
      "|Breezy Point/Fort...|          25|\n",
      "|Marine Park/Floyd...|          26|\n",
      "|        Astoria Park|          29|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Zone, COUNT(*) AS pickup_count\n",
    "FROM\n",
    "    trips\n",
    "GROUP BY Zone\n",
    "ORDER BY pickup_count\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907a93f-e798-4ee9-ae83-766e88a0d72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
